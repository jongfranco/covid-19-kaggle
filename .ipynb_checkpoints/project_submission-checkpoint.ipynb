{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fighting COVID-19 Infodemics \n",
    "## Project Design\n",
    "\n",
    "Knowledge gap and uncertainties between general public and specialists are important factors that drive pandemic anxiety. In this Kaggle competition task, we will examine the papers that discuss some of the controversial topics that trigger social rumours and pandenic anxiety in general public during the spread of the pandemic covid-19. \n",
    "\n",
    "Ever since the beginning of 2020, there have been are lots of unverified statements about the virus and how to prevent it from spreading circulating in many social media. One way to validate these statements is to examine the relevant academic papers. However, most of the public are struggling to find out the right keywords to search the academic papers, and they are usually reluctant to go through the entire papers to search for the relevant information. In this task, we aim to develop a searching system to extract the sentences from abstracts in the CORD-91 dataset that are relevant to evaluate a specific topic.\n",
    "\n",
    "For each statment, we first use the system to extract the topic relevant sentences from the paper abstracts, and then manually annotate those sentences based on whether they are for or against the statement.\n",
    "\n",
    "Here are some of the example statements about covid-19:\n",
    "\n",
    "1. Wearing mask is an effective way to stop the spreading of the virus\n",
    "\n",
    "2. The incubation period of covid-19 ranges from 2-14 days with a median of 5 days\n",
    "\n",
    "3. Asymptomatic patients can drive the spread of the virus\n",
    "\n",
    "For each statement, we define at least two stance levels according to whether the exatracted sentences in the abstracts support or oppose the statement. Here are the steps to achieve this goal:\n",
    "\n",
    "### Step 1: Developing a topic searching system \n",
    "\n",
    "The searching system first extracts abstracts contain a particular keyword (e.g. ‘mask’), then we use LDA to group the abstract topics. We identify a topic that is the most relevant to the statement and extract the abstracts that contain the target topic. The system then extract the sentences that contain the keyword from the relevant abstracts. The standard apporach of a searching system is to use TFIDF to rank documents, here we use LDA topic modeling on nouns, verbs, and adjectives of the abstracts. Users can decide the relevant information when knowing what are the most frequent keywords in each topic. For some queries, users want to identify articles for covid-19 only. Therefore, users are opt to add a title filer for different queries in the system.\n",
    "\n",
    "The benefit of this approach is that when we want to know the relevant contents for a question, we don't know beforehand what are the keywords in an article that are more relevant to the question we ask, because users from general public are usually not farmiliar with academic papers. In our system, the topic keywords serve as the primes for the query in the next step of sentence extraction in the abstracts.\n",
    "\n",
    "### Step 2: Mannually annotating stance and relevance\n",
    "\n",
    "We manually annotate the key sentences to identify the stance of the result sentences and whether these sentences are relevant to the question asked. Relevance annotation is an important part for evaluating a searching system.\n",
    "\n",
    "\n",
    "#### Annotation results\n",
    "To understand the answer to the relevant question, we need to annotate the stance of the results, e.g., whether the abstract is for / against the statement. \n",
    "\n",
    "To evaluate the searching system, we need to annotate the relevance of the retrieved sentences. Please refer to each section for the annotation guildlines\n",
    "\n",
    "Retrieved results and annotations can be found in this document \n",
    "https://docs.google.com/spreadsheets/d/1-eWEqji7mLXNF0Z9KH8RE5djcxK-97dUHzPWY7GEhI8/edit?usp=sharing\n",
    "\n",
    "The document contains:\n",
    "\n",
    "1. Annotation of stance:\n",
    "\n",
    "See the column 'stance' in the sheets 'mask', 'incubation', 'asymtomatic', \n",
    "\n",
    "2. Annotation for relevance\n",
    "\n",
    "See the column 'relevance' in the sheets 'mask', 'incubation', 'asymtomatic', \n",
    "\n",
    "3. Annotation for system evaluation \n",
    "\n",
    "See the column 'relevance' in the sheets 'system_eval_varname'. \n",
    "\n",
    "\n",
    "### Results:\n",
    "\n",
    "#### Statement 1: Wearing mask is an effective way to stop the spreading of the virus\n",
    "\n",
    "According to the key sentences in the 40 paper abstracts that discuss the topic of public using masks, 12 papers support that using a mask during a pandemic is useful, 18 papers assume masks to be useful and examine the public’s willingness to comply the mask wearing rules, 1 paper shows no obvious evidence that using mask is protective or the protection is very limited.\n",
    "\n",
    "We also see that the governments in some regions advocate using masks as a standard approach to reduce the risk of infection, papers in these regions focus on whether people comply the rules. Some government advocate that there is little evidence showing that mask is effective in controlling the pandemic, whereas nearly half of the academic papers from our search results consider wearing masks as a standard practice and these papers examine whether the public comply the recommended practice. Another half of the papers found evidence to support that wearing masks is effective in controlling the pandemic.\n",
    "\n",
    "\n",
    "### Statement 2: The incubation period range from 2-14 days with a media of 5 days\n",
    "\n",
    "There are 16 papers showing that the incubation period of covid-19 is 2-14 days with median of 5 days, 51 papers show different numbers. We can see that the majority of the papers show slightly different results than what the authorities reported. \n",
    "\n",
    "#### Statement 3: Asymptomatic patients drive the spread of the virus\n",
    "\n",
    "According to 53 papers relevant to this topic, 28 papers show that there is clear evidence that asymtomatic cases contribute to the spread of the virus, 25 papers show that it is unlikely that asymtomatic cases contribute to the spread of the virus. Therefore, more research is needed for this topic. As general public, one need to take some precautions when contacting others who might be asymptomatic patients.\n",
    "\n",
    "\n",
    "### Evaluation of the system:\n",
    "\n",
    "The evaluation of the system is currently being implemented. A test collection will be annotated in order to compare our search system with a baseline system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Searching System "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from collections import defaultdict\n",
    "import string\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim\n",
    "from pprint import pprint\n",
    "import spacy,en_core_web_sm\n",
    "from nltk.stem import PorterStemmer\n",
    "import os\n",
    "import json\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import re\n",
    "import collections\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the metadata in a dictionary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaData:\n",
    "    def __init__(self):\n",
    "        \"\"\"Define varibles.\"\"\"\n",
    "        # path and data\n",
    "        self.path = '/afs/inf.ed.ac.uk/user/s16/s1690903/share/cov19_2/'\n",
    "        self.meta_data = pd.read_csv(self.path + 'metadata.csv')\n",
    "\n",
    "    def data_dict(self):\n",
    "        \"\"\"Convert df to dictionary. \"\"\"\n",
    "        mydict = lambda: defaultdict(mydict)\n",
    "        meta_data_dict = mydict()\n",
    "\n",
    "        for cord_uid, abstract, title, sha in zip(self.meta_data['cord_uid'], self.meta_data['abstract'], self.meta_data['title'], self.meta_data['sha']):\n",
    "            meta_data_dict[cord_uid]['title'] = title\n",
    "            meta_data_dict[cord_uid]['abstract'] = abstract\n",
    "            meta_data_dict[cord_uid]['sha'] = sha\n",
    "\n",
    "        return meta_data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract documents containing keywords, preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractText:\n",
    "    \"\"\"Extract text according to keywords or phrases\"\"\"\n",
    "\n",
    "    def __init__(self, metaDict, keyword, variable):\n",
    "        \"\"\"Define varibles.\"\"\"\n",
    "        self.path = '/afs/inf.ed.ac.uk/user/s16/s1690903/share/cov19_2/'\n",
    "        self.metadata = metaDict\n",
    "        self.keyword = keyword\n",
    "        self.variable = variable\n",
    "\n",
    "\n",
    "    def simple_preprocess(self):\n",
    "        \"\"\"Simple text process: lower case, remove punc. \"\"\"\n",
    "        mydict = lambda: defaultdict(mydict)\n",
    "        cleaned = mydict()\n",
    "        for k, v in self.metadata.items():\n",
    "            sent = v[self.variable]\n",
    "            sent = str(sent).lower().translate(str.maketrans('', '', string.punctuation))\n",
    "            cleaned[k]['processed_text'] = sent\n",
    "            cleaned[k]['sha'] = v['sha']\n",
    "            cleaned[k]['title'] = v['title']\n",
    "\n",
    "        return cleaned\n",
    "\n",
    "    def very_simple_preprocess(self):\n",
    "        \"\"\"Simple text process: lower case only. \"\"\"\n",
    "        mydict = lambda: defaultdict(mydict)\n",
    "        cleaned = mydict()\n",
    "        for k, v in self.metadata.items():\n",
    "            sent = v[self.variable]\n",
    "            sent = str(sent)\n",
    "            #sent = str(sent).lower()\n",
    "            cleaned[k]['processed_text'] = sent\n",
    "            cleaned[k]['sha'] = v['sha']\n",
    "            cleaned[k]['title'] = v['title']\n",
    "\n",
    "        return cleaned\n",
    "     \n",
    "\n",
    "    def extract_w_keywords(self):\n",
    "        \"\"\"Select content with keywords.\"\"\"\n",
    "        ps = PorterStemmer()\n",
    "        mydict = lambda: defaultdict(mydict)\n",
    "        selected = mydict()\n",
    "        textdict = self.simple_preprocess()\n",
    "        \n",
    "        for k, v in textdict.items():\n",
    "            if self.keyword in v['processed_text'].split():\n",
    "                #print(ps.stem(str(self.keyword)))\n",
    "                selected[k]['processed_text'] = v['processed_text']\n",
    "                selected[k]['sha'] = v['sha']\n",
    "                selected[k]['title'] = v['title']\n",
    "        return selected\n",
    "\n",
    "    def extract_w_keywords_punc(self):\n",
    "        \"\"\"Select content with keywords, with punctuations in text\"\"\"\n",
    "        ps = PorterStemmer()\n",
    "        mydict = lambda: defaultdict(mydict)\n",
    "        selected = mydict()\n",
    "        textdict = self.very_simple_preprocess()\n",
    "        \n",
    "        for k, v in textdict.items():\n",
    "            #keywords are stemmed before matching\n",
    "            if ps.stem(str(self.keyword)) in ps.stem(str(v['processed_text'].split())):\n",
    "                selected[k]['processed_text'] = v['processed_text']\n",
    "                selected[k]['sha'] = v['sha']\n",
    "                selected[k]['title'] = v['title']\n",
    "        return selected\n",
    "\n",
    "    def get_noun_verb(self, text):\n",
    "        \"\"\"get noun trunks for the lda model,\n",
    "        change noun and verb part to decide what\n",
    "        you want to use as input for LDA\"\"\"\n",
    "        ps = PorterStemmer()\n",
    "      \n",
    "        #find nound trunks\n",
    "        nlp = en_core_web_sm.load()\n",
    "        all_extracted = {}\n",
    "        for k, v in text.items():\n",
    "            #v = v.replace('incubation period', 'incubation_period')\n",
    "            doc = nlp(v)\n",
    "            nouns = ' '.join(str(v) for v in doc if v.pos_ is 'NOUN').split()\n",
    "            verbs = ' '.join(ps.stem(str(v)) for v in doc if v.pos_ is 'VERB').split()\n",
    "            adj = ' '.join(str(v) for v in doc if v.pos_ is 'ADJ').split()\n",
    "            all_w = nouns + verbs + adj\n",
    "            all_extracted[k] = all_w\n",
    "      \n",
    "        return all_extracted\n",
    "\n",
    "    def get_noun_verb2(self, text):\n",
    "        \"\"\"get noun trunks for the lda model,\n",
    "        change noun and verb part to decide what\n",
    "        you want to use as input for LDA\"\"\"\n",
    "        ps = PorterStemmer()\n",
    "      \n",
    "        #find nound trunks\n",
    "        nlp = en_core_web_sm.load()\n",
    "        all_extracted = {}\n",
    "        for k, v in text.items():\n",
    "            #v = v.replace('incubation period', 'incubation_period')\n",
    "            doc = nlp(v['processed_text'])\n",
    "            nouns = ' '.join(ps.stem(str(v)) for v in doc if v.pos_ is 'NOUN').split()\n",
    "            verbs = ' '.join(ps.stem(str(v)) for v in doc if v.pos_ is 'VERB').split()\n",
    "            adj = ' '.join(str(v) for v in doc if v.pos_ is 'ADJ').split()\n",
    "            all_w = nouns + verbs + adj\n",
    "            all_extracted[k] = all_w\n",
    "      \n",
    "        return all_extracted\n",
    "\n",
    "    def tokenization(self, text):\n",
    "        \"\"\"get noun trunks for the lda model,\n",
    "        change noun and verb part to decide what\n",
    "        you want to use as input for the next step\"\"\"\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "        all_extracted = {}\n",
    "        for k, v in text.items():\n",
    "            doc = nlp(v)\n",
    "            all_extracted[k] = [w.text for w in doc]\n",
    "      \n",
    "        return all_extracted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LDA to rank documents\n",
    "LDA is optimized by coherence score u_mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDATopic:\n",
    "    def __init__(self, processed_text, topic_num, alpha, eta):\n",
    "        \"\"\"Define varibles.\"\"\"\n",
    "        self.path = '/afs/inf.ed.ac.uk/user/s16/s1690903/share/cov19_2/'\n",
    "        self.text = processed_text\n",
    "        self.topic_num = topic_num\n",
    "        self.alpha = alpha\n",
    "        self.eta = eta\n",
    "\n",
    "    def get_lda_score_eval(self, dictionary, bow_corpus):\n",
    "        \"\"\"LDA model and coherence score.\"\"\"\n",
    "\n",
    "        lda_model = gensim.models.ldamodel.LdaModel(bow_corpus, num_topics=self.topic_num, id2word=dictionary, passes=10,  update_every=1, random_state = 300, alpha=self.alpha, eta=self.eta)\n",
    "        #pprint(lda_model.print_topics())\n",
    "\n",
    "        # get coherence score\n",
    "        cm = CoherenceModel(model=lda_model, corpus=bow_corpus, coherence='u_mass')\n",
    "        coherence = cm.get_coherence()\n",
    "        print('coherence score is {}'.format(coherence))\n",
    "\n",
    "        return lda_model, coherence\n",
    "\n",
    "    def get_score_dict(self, bow_corpus, lda_model_object):\n",
    "        \"\"\"\n",
    "        get lda score for each document\n",
    "        \"\"\"\n",
    "        all_lda_score = {}\n",
    "        for i in range(len(bow_corpus)):\n",
    "            lda_score ={}\n",
    "            for index, score in sorted(lda_model_object[bow_corpus[i]], key=lambda tup: -1*tup[1]):\n",
    "                lda_score[index] = score\n",
    "                od = collections.OrderedDict(sorted(lda_score.items()))\n",
    "            all_lda_score[i] = od\n",
    "        return all_lda_score\n",
    "\n",
    "\n",
    "    def topic_modeling(self):\n",
    "        \"\"\"Get LDA topic modeling.\"\"\"\n",
    "        # generate dictionary\n",
    "        dictionary = gensim.corpora.Dictionary(self.text.values())\n",
    "        bow_corpus = [dictionary.doc2bow(doc) for doc in self.text.values()]\n",
    "        # modeling\n",
    "        model, coherence = self.get_lda_score_eval(dictionary, bow_corpus)\n",
    "\n",
    "        lda_score_all = self.get_score_dict(bow_corpus, model)\n",
    "\n",
    "        all_lda_score_df = pd.DataFrame.from_dict(lda_score_all)\n",
    "        all_lda_score_dfT = all_lda_score_df.T\n",
    "        all_lda_score_dfT = all_lda_score_dfT.fillna(0)\n",
    "\n",
    "        return model, coherence, all_lda_score_dfT\n",
    "\n",
    "    def get_ids_from_selected(self, text):\n",
    "        \"\"\"Get unique id from text \"\"\"\n",
    "        id_l = []\n",
    "        for k, v in text.items():\n",
    "            id_l.append(k)\n",
    "            \n",
    "        return id_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select documents (abstract/ article body) according to search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchArticleBody:\n",
    "    def __init__(self, path, selected_id):\n",
    "        \"\"\"Define varibles.\"\"\"\n",
    "        self.path = path\n",
    "        self.selected_id = selected_id\n",
    "\n",
    "\n",
    "    def read_folder(self):\n",
    "        \"\"\"\n",
    "        Creates a nested dictionary that represents the folder structure of rootdir\n",
    "        \"\"\"\n",
    "        rootdir = self.path.rstrip(os.sep)\n",
    "\n",
    "        article_dict = {}\n",
    "        for path, dirs, files in os.walk(rootdir):\n",
    "            for f in files:\n",
    "                file_id = f.split('.')[0]\n",
    "                #print(file_id)\n",
    "                try:\n",
    "                # load json file according to id\n",
    "                    with open(self.path + f) as f:\n",
    "                        data = json.load(f)\n",
    "                except:\n",
    "                    pass\n",
    "                article_dict[file_id] = data\n",
    "\n",
    "        return article_dict\n",
    "\n",
    "\n",
    "    def extract_bodytext(self):\n",
    "        \"\"\"Unpack nested dictionary and extract body of the article\"\"\"\n",
    "        body = {}\n",
    "        article_dict = self.read_folder()\n",
    "        for k, v in article_dict.items():\n",
    "            strings = ''\n",
    "            prevString = ''\n",
    "            for entry in v['body_text']:\n",
    "                strings = strings + prevString\n",
    "                prevString = entry['text']\n",
    "\n",
    "            body[k] = strings\n",
    "        return body\n",
    "\n",
    "\n",
    "    def get_title_by_bodykv(self, article_dict, keyword):\n",
    "        \"\"\"Search keyword in article body and return title\"\"\"\n",
    "\n",
    "        article_dict = self.read_folder()\n",
    "        selected_id = self.extract_id_list()\n",
    "\n",
    "        result = {}\n",
    "        for k, v in article_dict.items():\n",
    "            for entry in v['body_text']:\n",
    "                if (keyword in entry['text'].split()) and (k in selected_id):\n",
    "                    result[k] = v['metadata']['title']\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    def extract_id_list(self):\n",
    "        \"\"\"Extract ids from the selected text. \"\"\"\n",
    "        selected_id = []\n",
    "        for k, v in self.selected_id.items():\n",
    "            selected_id.append(str(v['sha']).split(';')[0])\n",
    "            try:\n",
    "                selected_id.append(str(v['sha']).split(';')[1])\n",
    "                selected_id.append(str(v['sha']).split(';')[2])\n",
    "                selected_id.append(str(v['sha']).split(';')[3])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return selected_id\n",
    "\n",
    "\n",
    "    def select_text_w_id(self):\n",
    "        body_text = self.extract_bodytext()\n",
    "        selected_id = self.extract_id_list()\n",
    "        selected_text = {}\n",
    "        for k, v in body_text.items():\n",
    "            if k in selected_id:\n",
    "                selected_text[k] = v\n",
    "        return selected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we extract articles contain the most relevant topic\n",
    "\n",
    "def selected_best_LDA(keyword, varname):\n",
    "        \"\"\"Select the best lda model with extracted text \"\"\"\n",
    "        # convert data to dictionary format\n",
    "        m = MetaData()\n",
    "        metaDict = m.data_dict()\n",
    "\n",
    "        #process text and extract text with keywords\n",
    "        et = ExtractText(metaDict, keyword, varname)\n",
    "        text1 = et.extract_w_keywords()\n",
    "\n",
    "\n",
    "        # extract nouns, verbs and adjetives\n",
    "        text = et.get_noun_verb2(text1)\n",
    "\n",
    "        # optimized alpha and beta\n",
    "        alpha = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "        beta = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "        mydict = lambda: defaultdict(mydict)\n",
    "        cohere_dict = mydict()\n",
    "        for a in alpha:\n",
    "            for b in beta:\n",
    "                lda = LDATopic(text, 20, a, b)\n",
    "                model, coherence, scores = lda.topic_modeling()\n",
    "                cohere_dict[coherence]['a'] = a\n",
    "                cohere_dict[coherence]['b'] = b\n",
    "    \n",
    "        # sort result dictionary to identify the best a, b\n",
    "        # select a,b with the largest coherence score \n",
    "        sort = sorted(cohere_dict.keys())[0] \n",
    "        a = cohere_dict[sort]['a']\n",
    "        b = cohere_dict[sort]['b']\n",
    "        \n",
    "        # run LDA with the optimized values\n",
    "        lda = LDATopic(text, 20, a, b)\n",
    "        model, coherence, scores_best = lda.topic_modeling()\n",
    "        pprint(model.print_topics())\n",
    "\n",
    "        # select merge ids with the LDA topic scores\n",
    "        id_l = lda.get_ids_from_selected(text)\n",
    "        scores_best['cord_uid'] = id_l\n",
    "\n",
    "        return scores_best\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def select_text_from_LDA_results(keyword, varname, scores_best, topic_num):\n",
    "        # choose papers with the most relevant topic\n",
    "        # convert data to dictionary format\n",
    "        m = MetaData()\n",
    "        metaDict = m.data_dict()\n",
    "\n",
    "        # process text and extract text with keywords\n",
    "        et = ExtractText(metaDict, keyword, varname)\n",
    "        # extract text together with punctuation\n",
    "        text1 = et.extract_w_keywords_punc()\n",
    "        # need to decide which topic to choose after training\n",
    "        sel = scores_best[scores_best[topic_num] > 0] \n",
    "        \n",
    "        mydict = lambda: defaultdict(mydict)\n",
    "        selected = mydict()\n",
    "        for k, v in text1.items():\n",
    "            if k in sel.cord_uid.tolist():\n",
    "                selected[k]['title'] = v['title']\n",
    "                selected[k]['processed_text'] = v['processed_text']\n",
    "                selected[k]['sha'] = v['sha']\n",
    "    \n",
    "        return selected\n",
    "\n",
    "def extract_relevant_sentences(cor_dict, search_keywords, filter_title=None):\n",
    "    \"\"\"Extract sentences contain keyword in relevant articles. \"\"\"\n",
    "    #here user can also choose whether they would like to only select title contain covid keywords\n",
    "\n",
    "    mydict = lambda: defaultdict(mydict)\n",
    "    sel_sentence = mydict()\n",
    "    filter_w = ['covid19','ncov','2019-ncov','covid-19','sars-cov','wuhan']\n",
    "    \n",
    "    for k, v in cor_dict.items():\n",
    "        keyword_sentence = []\n",
    "        sentences = v['processed_text'].split('.')\n",
    "        for sentence in sentences:\n",
    "            # for each sentence, check if keyword exist\n",
    "            # append sentences contain keyword to list\n",
    "            keyword_sum = sum(1 for word in search_keywords if word in sentence)\n",
    "            if keyword_sum > 0:\n",
    "                keyword_sentence.append(sentence)         \n",
    "\n",
    "        # store results\n",
    "        if not keyword_sentence:\n",
    "            pass\n",
    "        elif filter_title is not None:\n",
    "            for f in filter_w:\n",
    "                title = v['title'].lower().translate(str.maketrans('', '', string.punctuation))\n",
    "                abstract = v['processed_text'].lower().translate(str.maketrans('', '', string.punctuation))\n",
    "                if (f in title) or (f in abstract):\n",
    "                    sel_sentence[k]['sentences'] = keyword_sentence\n",
    "                    sel_sentence[k]['sha'] = v['sha']\n",
    "                    sel_sentence[k]['title'] = v['title'] \n",
    "        else:\n",
    "            sel_sentence[k]['sentences'] = keyword_sentence\n",
    "            sel_sentence[k]['sha'] = v['sha']\n",
    "            sel_sentence[k]['title'] = v['title'] \n",
    "            \n",
    "    print('{} articles are relevant to the topic you choose'.format(len(sel_sentence)))\n",
    "\n",
    "    path = '/afs/inf.ed.ac.uk/user/s16/s1690903/share/cov19_2/'\n",
    "    df = pd.DataFrame.from_dict(sel_sentence, orient='index')\n",
    "    df.to_csv(path + 'search_results_{}.csv'.format(search_keywords))\n",
    "    sel_sentence_df = pd.read_csv(path + 'search_results_{}.csv'.format(search_keywords))\n",
    "    return sel_sentence, sel_sentence_df\n",
    "\n",
    "def extract_relevant_sentences2(cor_dict, search_keywords, filter_title=None):\n",
    "    \"\"\"Extract sentences contain keyword in relevant articles for system evaluation. \"\"\"\n",
    "    #here user can also choose whether they would like to only select title contain covid keywords\n",
    "    #difference from the previous one is where we store the result\n",
    "\n",
    "    mydict = lambda: defaultdict(mydict)\n",
    "    sel_sentence = mydict()\n",
    "    filter_w = ['covid19','ncov','2019-ncov','covid-19','sars-cov','wuhan']\n",
    "    \n",
    "    for k, v in cor_dict.items():\n",
    "        keyword_sentence = []\n",
    "        sentences = v['processed_text'].split('.')\n",
    "        for sentence in sentences:\n",
    "            # for each sentence, check if keyword exist\n",
    "            # append sentences contain keyword to list\n",
    "            keyword_sum = sum(1 for word in search_keywords if word in sentence)\n",
    "            if keyword_sum > 0:\n",
    "                keyword_sentence.append(sentence)         \n",
    "\n",
    "        # store results\n",
    "        if not keyword_sentence:\n",
    "            pass\n",
    "        \n",
    "        elif filter_title is not None:\n",
    "            for f in filter_w:\n",
    "                title = v['title'].lower().translate(str.maketrans('', '', string.punctuation))\n",
    "                abstract = v['processed_text'].lower().translate(str.maketrans('', '', string.punctuation))\n",
    "                if (f in title) or (f in abstract):\n",
    "                    sel_sentence[k]['sentences'] = keyword_sentence\n",
    "                    sel_sentence[k]['sha'] = v['sha']\n",
    "                    sel_sentence[k]['title'] = v['title'] \n",
    "        else:\n",
    "            sel_sentence[k]['sentences'] = keyword_sentence\n",
    "            sel_sentence[k]['sha'] = v['sha']\n",
    "            sel_sentence[k]['title'] = v['title'] \n",
    "    print('{} articles contain keyword {}'.format(len(sel_sentence),  search_keywords))\n",
    "\n",
    "    path = '/afs/inf.ed.ac.uk/user/s16/s1690903/share/cov19_2/eval/'\n",
    "    df = pd.DataFrame.from_dict(sel_sentence, orient='index')\n",
    "    df.to_csv(path + 'eval_results_{}.csv'.format(search_keywords))\n",
    "    sel_sentence_df = pd.read_csv(path + 'eval_results_{}.csv'.format(search_keywords))\n",
    "    return sel_sentence, sel_sentence_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 1: Is wearing mask an effective way to control the pandemic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s16/s1690903/share/new_conda_for_me/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/afs/inf.ed.ac.uk/user/s16/s1690903/share/new_conda_for_me/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/afs/inf.ed.ac.uk/user/s16/s1690903/share/new_conda_for_me/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/afs/inf.ed.ac.uk/user/s16/s1690903/share/new_conda_for_me/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/afs/inf.ed.ac.uk/user/s16/s1690903/share/new_conda_for_me/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/afs/inf.ed.ac.uk/user/s16/s1690903/share/new_conda_for_me/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coherence score is -4.336722639525242\n",
      "coherence score is -3.9010410239440367\n",
      "coherence score is -4.389157036678158\n",
      "coherence score is -4.585109650895275\n",
      "coherence score is -4.658858479495897\n",
      "coherence score is -4.181983373771045\n",
      "coherence score is -3.8225572164182773\n",
      "coherence score is -4.115948791902841\n",
      "coherence score is -4.405996653207033\n",
      "coherence score is -4.893575872869723\n",
      "coherence score is -3.763406611071013\n",
      "coherence score is -4.054636612549744\n",
      "coherence score is -3.974976478023649\n",
      "coherence score is -3.605098511028769\n",
      "coherence score is -3.218469889363284\n",
      "coherence score is -3.922771443759005\n",
      "coherence score is -4.56961216145051\n",
      "coherence score is -2.9539869626024027\n",
      "coherence score is -3.2776086868166603\n",
      "coherence score is -3.6310570980589993\n",
      "coherence score is -3.8129640801122164\n",
      "coherence score is -3.7657510362336026\n",
      "coherence score is -3.4720471247415454\n",
      "coherence score is -3.0704332896336615\n",
      "coherence score is -2.990024667297296\n",
      "coherence score is -4.893575872869723\n",
      "[(0,\n",
      "  '0.012*\"patient\" + 0.009*\"use\" + 0.009*\"recommend\" + 0.008*\"mask\" + '\n",
      "  '0.007*\"ventil\" + 0.006*\"acute\" + 0.005*\"respiratory\" + 0.004*\"failur\" + '\n",
      "  '0.004*\"niv\" + 0.003*\"intub\"'),\n",
      " (1,\n",
      "  '0.010*\"mask\" + 0.007*\"public\" + 0.007*\"infect\" + 0.006*\"use\" + 0.006*\"wear\" '\n",
      "  '+ 0.005*\"studi\" + 0.005*\"measur\" + 0.005*\"face\" + 0.005*\"behavior\" + '\n",
      "  '0.005*\"health\"'),\n",
      " (2,\n",
      "  '0.004*\"particl\" + 0.003*\"valu\" + 0.003*\"penetr\" + 0.003*\"size\" + '\n",
      "  '0.003*\"filter\" + 0.002*\"mask\" + 0.002*\"leak\" + 0.002*\"gauz\" + 0.002*\"model\" '\n",
      "  '+ 0.002*\"method\"'),\n",
      " (3,\n",
      "  '0.003*\"epitop\" + 0.002*\"protect\" + 0.002*\"vaccin\" + 0.002*\"skin\" + '\n",
      "  '0.001*\"immunogen\" + 0.001*\"neutral\" + 0.001*\"membran\" + 0.001*\"mucous\" + '\n",
      "  '0.001*\"design\" + 0.001*\"immune\"'),\n",
      " (4,\n",
      "  '0.004*\"viral\" + 0.003*\"psychological\" + 0.003*\"citi\" + 0.002*\"data\" + '\n",
      "  '0.002*\"such\" + 0.002*\"human\" + 0.002*\"impact\" + 0.002*\"virus\" + '\n",
      "  '0.002*\"sampl\" + 0.002*\"anxieti\"'),\n",
      " (5,\n",
      "  '0.011*\"mask\" + 0.008*\"air\" + 0.007*\"exhal\" + 0.006*\"cough\" + 0.005*\"oxygen\" '\n",
      "  '+ 0.005*\"smoke\" + 0.005*\"dispers\" + 0.004*\"patient\" + 0.004*\"leakag\" + '\n",
      "  '0.004*\"plume\"'),\n",
      " (6,\n",
      "  '0.026*\"mask\" + 0.010*\"infect\" + 0.009*\"use\" + 0.007*\"respir\" + '\n",
      "  '0.007*\"group\" + 0.007*\"respiratory\" + 0.006*\"protect\" + 0.005*\"medical\" + '\n",
      "  '0.005*\"control\" + 0.005*\"test\"'),\n",
      " (7,\n",
      "  '0.002*\"activ\" + 0.002*\"properti\" + 0.002*\"eo\" + 0.001*\"antimicrobial\" + '\n",
      "  '0.001*\"tgev\" + 0.001*\"acid\" + 0.001*\"mutant\" + 0.001*\"bind\" + '\n",
      "  '0.001*\"hemagglutin\" + 0.001*\"sialic\"'),\n",
      " (8,\n",
      "  '0.000*\"mask\" + 0.000*\"use\" + 0.000*\"air\" + 0.000*\"place\" + 0.000*\"acute\" + '\n",
      "  '0.000*\"patient\" + 0.000*\"travel\" + 0.000*\"respiratory\" + 0.000*\"studi\" + '\n",
      "  '0.000*\"infect\"'),\n",
      " (9,\n",
      "  '0.018*\"mask\" + 0.018*\"use\" + 0.013*\"patient\" + 0.011*\"infect\" + '\n",
      "  '0.009*\"respiratory\" + 0.006*\"measur\" + 0.006*\"transmiss\" + 0.006*\"studi\" + '\n",
      "  '0.006*\"care\" + 0.006*\"risk\"'),\n",
      " (10,\n",
      "  '0.016*\"air\" + 0.008*\"use\" + 0.007*\"cleaner\" + 0.006*\"inroom\" + 0.006*\"room\" '\n",
      "  '+ 0.005*\"technolog\" + 0.005*\"hepa\" + 0.005*\"infectious\" + 0.004*\"airborne\" '\n",
      "  '+ 0.004*\"uvgi\"'),\n",
      " (11,\n",
      "  '0.004*\"view\" + 0.003*\"obstruct\" + 0.002*\"assess\" + 0.001*\"level\" + '\n",
      "  '0.001*\"paramet\" + 0.001*\"simple\" + 0.001*\"sky\" + 0.001*\"premium\" + '\n",
      "  '0.001*\"unobstructed\" + 0.001*\"residential\"'),\n",
      " (12,\n",
      "  '0.003*\"model\" + 0.002*\"rat\" + 0.001*\"estim\" + 0.001*\"weight\" + 0.001*\"mice\" '\n",
      "  '+ 0.001*\"forecast\" + 0.001*\"encephalitis\" + 0.001*\"paramet\" + 0.001*\"error\" '\n",
      "  '+ 0.001*\"casepati\"'),\n",
      " (13,\n",
      "  '0.000*\"studi\" + 0.000*\"use\" + 0.000*\"mask\" + 0.000*\"capit\" + 0.000*\"social\" '\n",
      "  '+ 0.000*\"discoveri\" + 0.000*\"potential\" + 0.000*\"measur\" + 0.000*\"link\" + '\n",
      "  '0.000*\"reduc\"'),\n",
      " (14,\n",
      "  '0.005*\"household\" + 0.003*\"emerg\" + 0.003*\"prepared\" + 0.003*\"patient\" + '\n",
      "  '0.003*\"meningococcal\" + 0.002*\"wear\" + 0.002*\"adjust\" + 0.002*\"doctor\" + '\n",
      "  '0.002*\"consult\" + 0.002*\"primary\"'),\n",
      " (15,\n",
      "  '0.004*\"filter\" + 0.001*\"untreated\" + 0.001*\"surviv\" + 0.001*\"relative\" + '\n",
      "  '0.001*\"treat\" + 0.001*\"dialdehyd\" + 0.001*\"biocid\" + 0.001*\"inactiv\" + '\n",
      "  '0.001*\"viable\" + 0.001*\"effici\"'),\n",
      " (16,\n",
      "  '0.005*\"antibodi\" + 0.004*\"protein\" + 0.002*\"neutral\" + 0.002*\"sampl\" + '\n",
      "  '0.002*\"serum\" + 0.002*\"region\" + 0.002*\"antigen\" + 0.002*\"domain\" + '\n",
      "  '0.002*\"show\" + 0.002*\"immun\"'),\n",
      " (17,\n",
      "  '0.002*\"symptom\" + 0.002*\"physical\" + 0.002*\"flow\" + 0.002*\"stress\" + '\n",
      "  '0.002*\"psychological\" + 0.001*\"haze\" + 0.001*\"techniqu\" + 0.001*\"score\" + '\n",
      "  '0.001*\"total\" + 0.001*\"role\"'),\n",
      " (18,\n",
      "  '0.000*\"mask\" + 0.000*\"use\" + 0.000*\"infectious\" + 0.000*\"studi\" + '\n",
      "  '0.000*\"result\" + 0.000*\"measur\" + 0.000*\"method\" + 0.000*\"filter\" + '\n",
      "  '0.000*\"respiratory\" + 0.000*\"air\"'),\n",
      " (19,\n",
      "  '0.007*\"intervent\" + 0.006*\"studi\" + 0.005*\"influenza\" + 0.005*\"infect\" + '\n",
      "  '0.004*\"use\" + 0.004*\"contig\" + 0.004*\"rpe\" + 0.003*\"haplotyp\" + '\n",
      "  '0.003*\"data\" + 0.003*\"pandemic\"')]\n"
     ]
    }
   ],
   "source": [
    "#here we select the LDA model with the lowe\n",
    "scores_best_mask = selected_best_LDA('mask', 'abstract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170, 21)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_best_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We observe topic No. 1 is the most relevant to public wearing mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40 abstracts selected\n"
     ]
    }
   ],
   "source": [
    "# topic number 1 is most relevant to public wearing mask\n",
    "# which topic do you think is most relevant to your search\n",
    "cor_dict_mask = select_text_from_LDA_results('mask', 'abstract', scores_best_mask, 1)\n",
    "print (\"There are {} abstracts selected\". format(len(cor_dict_mask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 articles are relevant to the topic you choose\n"
     ]
    }
   ],
   "source": [
    "# extract relevant sentences  #search keywords can be a list\n",
    "sel_sentence_mask, sel_sentence_df_mask = extract_relevant_sentences(cor_dict_mask, ['mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentences</th>\n",
       "      <th>sha</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8o3l3rsf</td>\n",
       "      <td>[', escalatory quarantine, mask wearing when g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Effectiveness of control strategies for Corona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1mu1z4xd</td>\n",
       "      <td>[' Wearing a mask when going out and avoiding ...</td>\n",
       "      <td>5bb89950ec5a06e2b7f69b2a9c4213dda19b1ab0</td>\n",
       "      <td>Prediction of New Coronavirus Infection Based ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kkpaovhh</td>\n",
       "      <td>[' For symptomatic, unconfirmed patients, doct...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Covid-19: What’s the current advice for UK doc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ht88wu6s</td>\n",
       "      <td>[' CONCLUSION: To early end of the COVID-19 ep...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Estimating the reproductive number and the out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>le0ogx1s</td>\n",
       "      <td>[\"The army of the men of death, in John Bunyan...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A new recruit for the army of the men of death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nzh87aux</td>\n",
       "      <td>[' On the other hand, the model predicts that ...</td>\n",
       "      <td>9b7a0ad7b6c7f59e7a6cf1dc9d07912a273d19b5</td>\n",
       "      <td>The Waiting Time for Inter-Country Spread of P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>n2r4pzan</td>\n",
       "      <td>[', wearing face mask in public venues (73', '...</td>\n",
       "      <td>b7c8e73cf095e30552a32cea04a398331c55ab41</td>\n",
       "      <td>Anticipated and current preventive behaviors i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ywb9krdp</td>\n",
       "      <td>['2%), and wear a face mask (59']</td>\n",
       "      <td>16627f4c7134394da448b1417a771d13ad7cca4a</td>\n",
       "      <td>Pandemic influenza in Australia: Using telepho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bhnh2dq4</td>\n",
       "      <td>[' If an infected person will not use a mask a...</td>\n",
       "      <td>bb9f6cef633c9baf595daae5166b11f88c1271cb</td>\n",
       "      <td>Risk of transmission of airborne infection dur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>49xvz389</td>\n",
       "      <td>['3%) were carrying out one of prevention meas...</td>\n",
       "      <td>545def8771357b4cb2875f5795a0760e97534cc9</td>\n",
       "      <td>Knowledge and attitudes of university students...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0                                          sentences  \\\n",
       "0   8o3l3rsf  [', escalatory quarantine, mask wearing when g...   \n",
       "1   1mu1z4xd  [' Wearing a mask when going out and avoiding ...   \n",
       "2   kkpaovhh  [' For symptomatic, unconfirmed patients, doct...   \n",
       "3   ht88wu6s  [' CONCLUSION: To early end of the COVID-19 ep...   \n",
       "4   le0ogx1s  [\"The army of the men of death, in John Bunyan...   \n",
       "5   nzh87aux  [' On the other hand, the model predicts that ...   \n",
       "6   n2r4pzan  [', wearing face mask in public venues (73', '...   \n",
       "7   ywb9krdp                  ['2%), and wear a face mask (59']   \n",
       "8   bhnh2dq4  [' If an infected person will not use a mask a...   \n",
       "9   49xvz389  ['3%) were carrying out one of prevention meas...   \n",
       "\n",
       "                                        sha  \\\n",
       "0                                       NaN   \n",
       "1  5bb89950ec5a06e2b7f69b2a9c4213dda19b1ab0   \n",
       "2                                       NaN   \n",
       "3                                       NaN   \n",
       "4                                       NaN   \n",
       "5  9b7a0ad7b6c7f59e7a6cf1dc9d07912a273d19b5   \n",
       "6  b7c8e73cf095e30552a32cea04a398331c55ab41   \n",
       "7  16627f4c7134394da448b1417a771d13ad7cca4a   \n",
       "8  bb9f6cef633c9baf595daae5166b11f88c1271cb   \n",
       "9  545def8771357b4cb2875f5795a0760e97534cc9   \n",
       "\n",
       "                                               title  \n",
       "0  Effectiveness of control strategies for Corona...  \n",
       "1  Prediction of New Coronavirus Infection Based ...  \n",
       "2  Covid-19: What’s the current advice for UK doc...  \n",
       "3  Estimating the reproductive number and the out...  \n",
       "4     A new recruit for the army of the men of death  \n",
       "5  The Waiting Time for Inter-Country Spread of P...  \n",
       "6  Anticipated and current preventive behaviors i...  \n",
       "7  Pandemic influenza in Australia: Using telepho...  \n",
       "8  Risk of transmission of airborne infection dur...  \n",
       "9  Knowledge and attitudes of university students...  "
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read extracted article\n",
    "sel_sentence_df_mask.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Annotation guidline for question 1\n",
    "We extract 33 papers that are supposed to discuss whether using masks is useful. We annotate whether the key sentences suggest using mask can reduce the risk of infection.\n",
    "\n",
    "#### Stance Annotation \n",
    "* '1' sentences that support using a mask during a pandemic is useful \n",
    "* '2' papers that assume wearing masks is useful and examine the public’s willingness to comply the rules,\n",
    "* '0' no obvious evidence that shows using mask is protective or the protection is very little\n",
    "* '3' not relevant to the above stance\n",
    "\n",
    "#### relevance annotation\n",
    "* '1' the result is relevent to the question  \n",
    "* '0' the result is not relevant to the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we need to add the stats analysis \n",
    "path = '/afs/inf.ed.ac.uk/user/s16/s1690903/share/cov19_2/annotation/'\n",
    "annotation_mask = pd.read_csv(path + 'wear_mask.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 40 articles relevant to the topic\n"
     ]
    }
   ],
   "source": [
    "# view file\n",
    "annotation_mask.head(5)\n",
    "print('there are {} articles relevant to the topic'.format(annotation_mask.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "* '1' support using a mask during a pandemic is useful \n",
    "* '2' assume masks as useful and examine the public’s willingness to comply the rules,\n",
    "* '0' no obvious evidence that shows using mask is protective or the protection is very little\n",
    "* '3' not relevant to the above stance\n",
    "\n",
    "result from annotator 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    18\n",
       "1    12\n",
       "3     9\n",
       "0     1\n",
       "Name: stance, dtype: int64"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_mask['stance'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result from annotator 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    21\n",
       "3     9\n",
       "1     8\n",
       "0     2\n",
       "Name: stance.1, dtype: int64"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_mask['stance.1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_mask['stance'].value_counts()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('there are {} papers support that using a mask during a pandemic is useful, {} assume masks as useful and examine the public’s willingness to comply the mask wearing rules, {} papers show no obvious evidence that shows using mask is protective or the protection is very little'. format(str(annotation_mask['stance'].value_counts()[1]), str(annotation_mask['stance'].value_counts()[2]), annotation_mask['stance'].value_counts()[0]) )\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inter-rater repliability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931927133269415"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(annotation_mask['stance'], annotation_mask['stance.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot first author location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 34 papers relevant to the topic, 6 papers not relevant to the topic\n"
     ]
    }
   ],
   "source": [
    "mask = annotation_mask['relevance'].value_counts()\n",
    "print('there are {} papers relevant to the topic, {} papers not relevant to the topic'. format(mask[1], mask[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Results\n",
    "According to the key sentences in the 33 abstracts that discuss the topic of public using masks, only one paper suggests that there is not enough evidence that mask is useful.\n",
    "There are 14 papers that suggest their results show that using surgical masks during a pandemic is effective in reducing the risk of infection\n",
    "14 papers consider public use of masks is necessary in reducing the risk of infection, and these papers look at whether the public are willing to comply the rules.\n",
    "6 papers are not relevant to the topic\n",
    "\n",
    "## Conclusion:\n",
    "Government in some regions advocate that using masks is a standard approach to reduce the risk of infection, papers in these regions focus on whether people comply to the rules. Although some governments advocate that there is little evidence showing that mask is effective in controlling the pandemic, nearly half of the academic papers from our searched results either consider wearing masks is a standard practice that the public show comply, nearly half of the papers provide evidence to support that wearing masks is effective in controlling the pandemic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 2: How long is the incubation period of covid-19? In some region (e.g. China), there is a rumour circulating that the incubation period is longer than 14 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Annotation guideline for question 2:\n",
    "\n",
    "#### stance annotation\n",
    "Here we want to identify papers that report a result aligns with the incubation period reported by the governments\n",
    "UK government advocated: 2-14 days, median 5\n",
    "* '1' same as government advocate \n",
    "* '0' different from what the government\n",
    "* Not relevant to the question \n",
    "\n",
    "#### relevance annotation\n",
    "* '1' the result is relevent to the question  \n",
    "* '2' the result is not relevant to the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coherence score is -4.402954777107732\n",
      "coherence score is -7.622641764950241\n",
      "coherence score is -9.667138416026404\n",
      "coherence score is -8.840829560303707\n",
      "coherence score is -7.75012700862416\n",
      "coherence score is -4.690846825022652\n",
      "coherence score is -7.537936150644841\n",
      "coherence score is -8.204060613000161\n",
      "coherence score is -7.780107529965858\n",
      "coherence score is -7.243947472108755\n",
      "coherence score is -4.860236911790262\n",
      "coherence score is -6.818539222229658\n",
      "coherence score is -8.020048375633895\n",
      "coherence score is -6.931077713393968\n",
      "coherence score is -6.426623602144103\n",
      "coherence score is -4.8710456253984065\n",
      "coherence score is -6.629118614124333\n",
      "coherence score is -5.956842964386887\n",
      "coherence score is -5.355865377504925\n",
      "coherence score is -4.764257961119005\n",
      "coherence score is -4.585814626682113\n",
      "coherence score is -6.216045484929411\n",
      "coherence score is -5.112704786045706\n",
      "coherence score is -4.641328661092419\n",
      "coherence score is -3.607818546962199\n",
      "coherence score is -9.667138416026404\n",
      "[(0,\n",
      "  '0.029*\"case\" + 0.022*\"patient\" + 0.018*\"period\" + 0.016*\"incub\" + '\n",
      "  '0.016*\"day\" + 0.014*\"infect\" + 0.011*\"covid19\" + 0.009*\"estim\" + '\n",
      "  '0.009*\"transmiss\" + 0.007*\"hospit\"'),\n",
      " (1,\n",
      "  '0.002*\"educ\" + 0.002*\"crisi\" + 0.002*\"unit\" + 0.002*\"peopl\" + 0.002*\"prbc\" '\n",
      "  '+ 0.002*\"phtr\" + 0.001*\"impact\" + 0.001*\"c\" + 0.001*\"viru\" + '\n",
      "  '0.001*\"phosphatidylcholin\"'),\n",
      " (2,\n",
      "  '0.005*\"patient\" + 0.004*\"model\" + 0.004*\"case\" + 0.004*\"group\" + '\n",
      "  '0.003*\"incid\" + 0.003*\"student\" + 0.003*\"pain\" + 0.002*\"function\" + '\n",
      "  '0.002*\"time\" + 0.002*\"steroid\"'),\n",
      " (3,\n",
      "  '0.009*\"infect\" + 0.003*\"viru\" + 0.003*\"product\" + 0.003*\"p\" + '\n",
      "  '0.003*\"laboratori\" + 0.002*\"process\" + 0.002*\"incub\" + 0.002*\"sperm\" + '\n",
      "  '0.002*\"viral\" + 0.002*\"case\"'),\n",
      " (4,\n",
      "  '0.003*\"camel\" + 0.003*\"macaqu\" + 0.002*\"baboon\" + 0.002*\"cultur\" + '\n",
      "  '0.002*\"tissu\" + 0.002*\"explant\" + 0.002*\"penetr\" + 0.002*\"shfv\" + '\n",
      "  '0.002*\"merscov\" + 0.002*\"ferret\"'),\n",
      " (5,\n",
      "  '0.002*\"bacteria\" + 0.002*\"intestinal\" + 0.001*\"metabol\" + 0.001*\"metabolit\" '\n",
      "  '+ 0.001*\"saprophyt\" + 0.001*\"juic\" + 0.001*\"gastrointestinal\" + '\n",
      "  '0.001*\"trip\" + 0.001*\"parasit\" + 0.001*\"anticomplementary\"'),\n",
      " (6,\n",
      "  '0.002*\"mhv\" + 0.002*\"room\" + 0.001*\"pathway\" + 0.001*\"instrument\" + '\n",
      "  '0.001*\"dental\" + 0.001*\"ah7n9\" + 0.001*\"secretori\" + 0.001*\"constitutive\" + '\n",
      "  '0.001*\"highspe\" + 0.001*\"rotat\"'),\n",
      " (7,\n",
      "  '0.002*\"hotel\" + 0.002*\"environ\" + 0.001*\"industri\" + 0.001*\"nanocomplex\" + '\n",
      "  '0.001*\"formul\" + 0.001*\"surfac\" + 0.001*\"prepared\" + 0.001*\"vaginal\" + '\n",
      "  '0.001*\"plan\" + 0.001*\"disast\"'),\n",
      " (8,\n",
      "  '0.004*\"activ\" + 0.003*\"peptid\" + 0.002*\"chicken\" + 0.002*\"nkcell\" + '\n",
      "  '0.002*\"somni\" + 0.002*\"hcovoc43\" + 0.002*\"oxid\" + 0.002*\"accumul\" + '\n",
      "  '0.002*\"bovine\" + 0.001*\"astrocyt\"'),\n",
      " (9,\n",
      "  '0.003*\"delay\" + 0.002*\"system\" + 0.001*\"nonautonomous\" + 0.001*\"pattern\" + '\n",
      "  '0.001*\"spatial\" + 0.001*\"lyapunov\" + 0.001*\"chaotic\" + 0.001*\"perman\" + '\n",
      "  '0.001*\"numerical\" + 0.001*\"ture\"'),\n",
      " (10,\n",
      "  '0.027*\"cell\" + 0.019*\"protein\" + 0.009*\"express\" + 0.009*\"viru\" + '\n",
      "  '0.008*\"incub\" + 0.007*\"infect\" + 0.006*\"viral\" + 0.006*\"receptor\" + '\n",
      "  '0.005*\"bind\" + 0.005*\"activ\"'),\n",
      " (11,\n",
      "  '0.009*\"diseas\" + 0.008*\"viru\" + 0.007*\"infect\" + 0.007*\"transmiss\" + '\n",
      "  '0.007*\"incub\" + 0.006*\"outbreak\" + 0.006*\"studi\" + 0.006*\"use\" + '\n",
      "  '0.005*\"case\" + 0.005*\"caus\"'),\n",
      " (12,\n",
      "  '0.002*\"t\" + 0.002*\"apical\" + 0.002*\"protein\" + 0.002*\"polypeptid\" + '\n",
      "  '0.002*\"basolateral\" + 0.002*\"cell\" + 0.002*\"mice\" + 0.002*\"supernat\" + '\n",
      "  '0.002*\"memori\" + 0.001*\"purif\"'),\n",
      " (13,\n",
      "  '0.001*\"airborne\" + 0.001*\"indoor\" + 0.000*\"occup\" + '\n",
      "  '0.000*\"shortincubationperiod\" + 0.000*\"environ\" + 0.000*\"theoretical\" + '\n",
      "  '0.000*\"room\" + 0.000*\"ventil\" + 0.000*\"feather\" + 0.000*\"zoonos\"'),\n",
      " (14,\n",
      "  '0.003*\"parasit\" + 0.002*\"prolifer\" + 0.002*\"idv\" + 0.002*\"lymphocyt\" + '\n",
      "  '0.002*\"rat\" + 0.001*\"transcript\" + 0.001*\"nigra\" + 0.001*\"erythrocyt\" + '\n",
      "  '0.001*\"brain\" + 0.001*\"popul\"'),\n",
      " (15,\n",
      "  '0.014*\"cell\" + 0.011*\"viru\" + 0.006*\"activ\" + 0.004*\"ace2\" + 0.004*\"incub\" '\n",
      "  '+ 0.004*\"shed\" + 0.004*\"express\" + 0.004*\"inhibitor\" + 0.004*\"infect\" + '\n",
      "  '0.003*\"cftr\"'),\n",
      " (16,\n",
      "  '0.005*\"cell\" + 0.003*\"strain\" + 0.003*\"insulin\" + 0.002*\"hpev1\" + '\n",
      "  '0.002*\"endothelial\" + 0.002*\"peptid\" + 0.002*\"recombinant\" + 0.002*\"virus\" '\n",
      "  '+ 0.002*\"fibril\" + 0.002*\"result\"'),\n",
      " (17,\n",
      "  '0.014*\"infect\" + 0.013*\"cell\" + 0.013*\"viru\" + 0.013*\"incub\" + 0.009*\"use\" '\n",
      "  '+ 0.009*\"detect\" + 0.007*\"result\" + 0.007*\"viral\" + 0.006*\"studi\" + '\n",
      "  '0.006*\"diseas\"'),\n",
      " (18,\n",
      "  '0.002*\"cadmium\" + 0.002*\"calv\" + 0.002*\"salmonella\" + 0.002*\"student\" + '\n",
      "  '0.002*\"cdcl2\" + 0.001*\"rotaviru\" + 0.001*\"veterinary\" + 0.001*\"preventive\" '\n",
      "  '+ 0.001*\"toxic\" + 0.001*\"excret\"'),\n",
      " (19,\n",
      "  '0.001*\"safeti\" + 0.001*\"airway\" + 0.001*\"rtd1\" + 0.001*\"toler\" + '\n",
      "  '0.001*\"inflamm\" + 0.001*\"bronchial\" + 0.001*\"peptid\" + 0.001*\"stabil\" + '\n",
      "  '0.001*\"therapeutic\" + 0.000*\"α26\"')]\n"
     ]
    }
   ],
   "source": [
    "scores_best_incu = selected_best_LDA('incubation', 'abstract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 213 abstracts selected\n"
     ]
    }
   ],
   "source": [
    "# topic number 0 is most relevant to public wearing mask\n",
    "# which topic do you think is most relevant to your search\n",
    "cor_dict_incu = select_text_from_LDA_results('incubation', 'abstract', scores_best_incu, 0)\n",
    "print (\"There are {} abstracts selected\". format(len(cor_dict_incu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 articles are relevant to the topic you choose\n"
     ]
    }
   ],
   "source": [
    "# extract relevant sentences  #search keywords can be a list\n",
    "sel_sentence_incu, sel_sentence_df_incu = extract_relevant_sentences(cor_dict_incu, ['day'], 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentences</th>\n",
       "      <th>sha</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>h89scli5</td>\n",
       "      <td>[' For monitored individuals, we identified un...</td>\n",
       "      <td>b5161b031c7f720562e94735a018d1c3c8be3ae5</td>\n",
       "      <td>Quantifying the Risk and Cost of Active Monito...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ykofrn9i</td>\n",
       "      <td>[' Our results show that the incubation period...</td>\n",
       "      <td>cbc05d14c57b91081970a232ab83bc993f998fe2</td>\n",
       "      <td>Incubation Period and Other Epidemiological Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u8goc7io</td>\n",
       "      <td>['7, 95% CI) days, ranging from 2', '1 days (2']</td>\n",
       "      <td>12fac9aedb1a09a3922a3c084ce4723708e463d6</td>\n",
       "      <td>The incubation period of 2019-nCoV infections ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vspnuxz9</td>\n",
       "      <td>['0 days (95% credible interval [CrI]: 3', '6 ...</td>\n",
       "      <td>a1bff76ce360e8990b0a4ee2a5228a6e6e63d9c1</td>\n",
       "      <td>Serial interval of novel coronavirus (2019-nCo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ra3t6kmm</td>\n",
       "      <td>['9 days (95% credible interval [CrI], 2 days-...</td>\n",
       "      <td>c85f571a674c7fed0ccb9176e9cf9f3d3659ca32</td>\n",
       "      <td>Analysis of the epidemic growth of the early 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tovfd9lw</td>\n",
       "      <td>['0 days (range, 0 to 24', '0 days)']</td>\n",
       "      <td>dfb0fedbeed56bd2b795a67faab28295afc14c96</td>\n",
       "      <td>Clinical characteristics of 2019 novel coronav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>45g12waw</td>\n",
       "      <td>['4 days, and the R0 value is likely to be bet...</td>\n",
       "      <td>36a5f6d55d7c5f67d4344e36da0a72856ad3dda0</td>\n",
       "      <td>The Novel Coronavirus, 2019-nCoV, is Highly Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rcbw54xc</td>\n",
       "      <td>['5days', ' Cumulation number of patients at t...</td>\n",
       "      <td>57e01ad2a4961cd5cc6a3733f5f8c013a8946f3c</td>\n",
       "      <td>A model simulation study on effects of interve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fmymklz6</td>\n",
       "      <td>[' Incubation time ranged from one to twenty d...</td>\n",
       "      <td>f3ff1ecae96700f41b83d2a034a3a959428388b0</td>\n",
       "      <td>The cross-sectional study of hospitalized coro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dbzrd23n</td>\n",
       "      <td>['6) days and the mean onset-admission interva...</td>\n",
       "      <td>eb8ac60527db35b10881cb4fd86b8a6e21983d02</td>\n",
       "      <td>A descriptive study of the impact of diseases ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0                                          sentences  \\\n",
       "0   h89scli5  [' For monitored individuals, we identified un...   \n",
       "1   ykofrn9i  [' Our results show that the incubation period...   \n",
       "2   u8goc7io   ['7, 95% CI) days, ranging from 2', '1 days (2']   \n",
       "3   vspnuxz9  ['0 days (95% credible interval [CrI]: 3', '6 ...   \n",
       "4   ra3t6kmm  ['9 days (95% credible interval [CrI], 2 days-...   \n",
       "5   tovfd9lw              ['0 days (range, 0 to 24', '0 days)']   \n",
       "6   45g12waw  ['4 days, and the R0 value is likely to be bet...   \n",
       "7   rcbw54xc  ['5days', ' Cumulation number of patients at t...   \n",
       "8   fmymklz6  [' Incubation time ranged from one to twenty d...   \n",
       "9   dbzrd23n  ['6) days and the mean onset-admission interva...   \n",
       "\n",
       "                                        sha  \\\n",
       "0  b5161b031c7f720562e94735a018d1c3c8be3ae5   \n",
       "1  cbc05d14c57b91081970a232ab83bc993f998fe2   \n",
       "2  12fac9aedb1a09a3922a3c084ce4723708e463d6   \n",
       "3  a1bff76ce360e8990b0a4ee2a5228a6e6e63d9c1   \n",
       "4  c85f571a674c7fed0ccb9176e9cf9f3d3659ca32   \n",
       "5  dfb0fedbeed56bd2b795a67faab28295afc14c96   \n",
       "6  36a5f6d55d7c5f67d4344e36da0a72856ad3dda0   \n",
       "7  57e01ad2a4961cd5cc6a3733f5f8c013a8946f3c   \n",
       "8  f3ff1ecae96700f41b83d2a034a3a959428388b0   \n",
       "9  eb8ac60527db35b10881cb4fd86b8a6e21983d02   \n",
       "\n",
       "                                               title  \n",
       "0  Quantifying the Risk and Cost of Active Monito...  \n",
       "1  Incubation Period and Other Epidemiological Ch...  \n",
       "2  The incubation period of 2019-nCoV infections ...  \n",
       "3  Serial interval of novel coronavirus (2019-nCo...  \n",
       "4  Analysis of the epidemic growth of the early 2...  \n",
       "5  Clinical characteristics of 2019 novel coronav...  \n",
       "6  The Novel Coronavirus, 2019-nCoV, is Highly Co...  \n",
       "7  A model simulation study on effects of interve...  \n",
       "8  The cross-sectional study of hospitalized coro...  \n",
       "9  A descriptive study of the impact of diseases ...  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read extracted article\n",
    "sel_sentence_df_incu.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Statistical analysis on the incubation period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 67 articles relevant to the topic\n"
     ]
    }
   ],
   "source": [
    "#here we need to add the stats analysis \n",
    "path = '/afs/inf.ed.ac.uk/user/s16/s1690903/share/cov19_2/annotation/'\n",
    "annotation_incubation = pd.read_csv(path + 'incubation.csv')\n",
    "print('there are {} articles relevant to the topic'.format(annotation_incubation.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result from annotator 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 51 paper shows the incubation period is 2-14 days with mean 5 days, 16 papers shows a different number\n"
     ]
    }
   ],
   "source": [
    "incubation = annotation_incubation['stance'].value_counts()\n",
    "print('there are {} papers showing that the incubation period is 2-14 days with a median of 5 days, {} papers show different numbers'. format(incubation[0], incubation[1])\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 51 papers relevant to the topic, 16 papers not relevant to the topic\n"
     ]
    }
   ],
   "source": [
    "incubation = annotation_incubation['relevance'].value_counts()\n",
    "print('there are {} papers relevant to the topic, {} papers not relevant to the topic'. format(incubation[1], incubation[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Are asymptomatic patients infectious?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Annotation guideline for question 3:\n",
    "Here we want to identify whether asymtomatic cases contribute to the spread of the virus\n",
    "\n",
    "#### stance annotation\n",
    "* '1' there is clear evidence show that asymtomatic cases contribute to the spread of the virus\n",
    "* '0' it is unlikely that asymtomatic cases contribute to the spread of the virus\n",
    "* '3' not relevant to the question\n",
    "\n",
    "#### relevance annotation\n",
    "* '1' the result is relevent to the question  \n",
    "* '0' the result is not relevant to the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coherence score is -2.333875362764049\n",
      "coherence score is -4.70447389062267\n",
      "coherence score is -6.939599491635526\n",
      "coherence score is -8.440505363564451\n",
      "coherence score is -9.591134816334584\n",
      "coherence score is -2.2594033960424147\n",
      "coherence score is -5.105876829676597\n",
      "coherence score is -6.9371475741261035\n",
      "coherence score is -8.606789439278637\n",
      "coherence score is -9.453634935286304\n",
      "coherence score is -2.638911020212841\n",
      "coherence score is -4.846954903243043\n",
      "coherence score is -6.8515540183404955\n",
      "coherence score is -8.924401332546264\n",
      "coherence score is -8.268359620682485\n",
      "coherence score is -2.4049378700075303\n",
      "coherence score is -4.651142459964543\n",
      "coherence score is -6.56690398681833\n",
      "coherence score is -7.81408598379605\n",
      "coherence score is -5.9898250531948944\n",
      "coherence score is -2.334124552642998\n",
      "coherence score is -4.078897796588015\n",
      "coherence score is -6.8391754668949485\n",
      "coherence score is -6.1568698549644205\n",
      "coherence score is -4.795885092862488\n",
      "coherence score is -9.591134816334584\n",
      "[(0,\n",
      "  '0.002*\"calv\" + 0.001*\"ioae\" + 0.001*\"stent\" + 0.001*\"graft\" + '\n",
      "  '0.001*\"fenestrated\" + 0.001*\"function\" + 0.001*\"branch\" + 0.001*\"bovin\" + '\n",
      "  '0.001*\"moderate\" + 0.001*\"lung\"'),\n",
      " (1,\n",
      "  '0.000*\"cell\" + 0.000*\"infect\" + 0.000*\"clinical\" + 0.000*\"viru\" + '\n",
      "  '0.000*\"diseas\" + 0.000*\"asymptomatic\" + 0.000*\"macaqu\" + 0.000*\"case\" + '\n",
      "  '0.000*\"allergic\" + 0.000*\"strain\"'),\n",
      " (2,\n",
      "  '0.001*\"yield\" + 0.001*\"biomark\" + 0.001*\"apoa1\" + 0.000*\"alveolar\" + '\n",
      "  '0.000*\"fob\" + 0.000*\"reticular\" + 0.000*\"diagnostic\" + 0.000*\"nodular\" + '\n",
      "  '0.000*\"fulllength\" + 0.000*\"infiltr\"'),\n",
      " (3,\n",
      "  '0.009*\"children\" + 0.008*\"patient\" + 0.008*\"allergic\" + 0.006*\"effect\" + '\n",
      "  '0.005*\"young\" + 0.005*\"case\" + 0.005*\"model\" + 0.004*\"asthma\" + '\n",
      "  '0.004*\"clinical\" + 0.004*\"treatment\"'),\n",
      " (4,\n",
      "  '0.009*\"cell\" + 0.008*\"bat\" + 0.006*\"respons\" + 0.005*\"infect\" + '\n",
      "  '0.004*\"immune\" + 0.004*\"dog\" + 0.004*\"viral\" + 0.003*\"gene\" + '\n",
      "  '0.003*\"protein\" + 0.002*\"macaqu\"'),\n",
      " (5,\n",
      "  '0.002*\"inflamm\" + 0.002*\"intraamniotic\" + 0.001*\"r\" + 0.001*\"piglet\" + '\n",
      "  '0.001*\"estim\" + 0.001*\"day\" + 0.001*\"method\" + 0.001*\"astrovirus\" + '\n",
      "  '0.001*\"oper\" + 0.001*\"cadmium\"'),\n",
      " (6,\n",
      "  '0.001*\"plant\" + 0.001*\"lifestyl\" + 0.000*\"influenza\" + 0.000*\"persistent\" + '\n",
      "  '0.000*\"virus\" + 0.000*\"eukaryotic\" + 0.000*\"partitivirida\" + '\n",
      "  '0.000*\"endornaviru\" + 0.000*\"vast\" + 0.000*\"wellcharacterized\"'),\n",
      " (7,\n",
      "  '0.001*\"ccv\" + 0.001*\"renal\" + 0.001*\"hypouricaemia\" + 0.001*\"urate\" + '\n",
      "  '0.000*\"uric\" + 0.000*\"stone\" + 0.000*\"inherit\" + 0.000*\"urat1\" + '\n",
      "  '0.000*\"cat\" + 0.000*\"handl\"'),\n",
      " (8,\n",
      "  '0.003*\"case\" + 0.001*\"avid\" + 0.001*\"sleep\" + 0.001*\"chest\" + 0.001*\"ct\" + '\n",
      "  '0.001*\"infect\" + 0.001*\"enfant\" + 0.001*\"patient\" + 0.001*\"imag\" + '\n",
      "  '0.001*\"acid\"'),\n",
      " (9,\n",
      "  '0.002*\"cage\" + 0.002*\"dog\" + 0.002*\"ecov\" + 0.002*\"hors\" + 0.001*\"piglet\" + '\n",
      "  '0.001*\"pheasant\" + 0.001*\"pid\" + 0.001*\"swab\" + 0.001*\"infect\" + '\n",
      "  '0.001*\"autoantibodi\"'),\n",
      " (10,\n",
      "  '0.002*\"blockag\" + 0.002*\"traffic\" + 0.001*\"market\" + 0.001*\"wet\" + '\n",
      "  '0.001*\"puuv\" + 0.001*\"quarantin\" + 0.001*\"id99\" + 0.001*\"htnv\" + '\n",
      "  '0.001*\"shrew\" + 0.001*\"hamster\"'),\n",
      " (11,\n",
      "  '0.001*\"mscpv1\" + 0.001*\"env\" + 0.001*\"bat\" + 0.001*\"wild\" + 0.001*\"pfv\" + '\n",
      "  '0.001*\"carnivor\" + 0.001*\"cakov\" + 0.001*\"kobuvirus\" + 0.001*\"gp48tm\" + '\n",
      "  '0.001*\"helic\"'),\n",
      " (12,\n",
      "  '0.001*\"puppi\" + 0.001*\"clean\" + 0.001*\"hcw\" + 0.001*\"rct\" + 0.001*\"pkdl\" + '\n",
      "  '0.001*\"room\" + 0.001*\"cpv\" + 0.001*\"ccov\" + 0.001*\"diarrhoea\" + '\n",
      "  '0.001*\"infector\"'),\n",
      " (13,\n",
      "  '0.001*\"proteas\" + 0.001*\"inhibitor\" + 0.001*\"gut\" + 0.001*\"3cl\" + '\n",
      "  '0.001*\"microbiom\" + 0.001*\"felin\" + 0.001*\"astvinfect\" + 0.000*\"enteric\" + '\n",
      "  '0.000*\"taxa\" + 0.000*\"astv\"'),\n",
      " (14,\n",
      "  '0.001*\"afadr\" + 0.001*\"fshd\" + 0.001*\"cbc\" + 0.001*\"express\" + '\n",
      "  '0.001*\"intestinal\" + 0.001*\"abund\" + 0.001*\"dux4fl\" + 0.001*\"skeletal\" + '\n",
      "  '0.001*\"fshdlike\" + 0.001*\"cytokin\"'),\n",
      " (15,\n",
      "  '0.001*\"student\" + 0.001*\"offspr\" + 0.001*\"tonsil\" + 0.001*\"cmml\" + '\n",
      "  '0.001*\"israeli\" + 0.001*\"obstruct\" + 0.001*\"btv3\" + 0.001*\"reassort\" + '\n",
      "  '0.001*\"bovi\" + 0.001*\"adenoid\"'),\n",
      " (16,\n",
      "  '0.002*\"bat\" + 0.002*\"model\" + 0.002*\"infect\" + 0.002*\"screen\" + '\n",
      "  '0.002*\"dromedari\" + 0.002*\"estim\" + 0.001*\"virus\" + 0.001*\"camel\" + '\n",
      "  '0.001*\"mhc\" + 0.001*\"peptid\"'),\n",
      " (17,\n",
      "  '0.017*\"infect\" + 0.015*\"respiratory\" + 0.014*\"detect\" + 0.013*\"patient\" + '\n",
      "  '0.013*\"asymptomatic\" + 0.012*\"children\" + 0.010*\"viru\" + 0.009*\"studi\" + '\n",
      "  '0.009*\"sampl\" + 0.009*\"virus\"'),\n",
      " (18,\n",
      "  '0.001*\"ipd\" + 0.001*\"mhva59\" + 0.001*\"microbiota\" + 0.001*\"chadox1\" + '\n",
      "  '0.001*\"homozygous\" + 0.000*\"children\" + 0.000*\"f11\" + 0.000*\"immunogen\" + '\n",
      "  '0.000*\"candid\" + 0.000*\"pfu\"'),\n",
      " (19,\n",
      "  '0.025*\"infect\" + 0.014*\"case\" + 0.011*\"asymptomatic\" + 0.011*\"diseas\" + '\n",
      "  '0.010*\"patient\" + 0.010*\"viru\" + 0.008*\"transmiss\" + 0.005*\"viral\" + '\n",
      "  '0.005*\"severe\" + 0.005*\"clinical\"')]\n"
     ]
    }
   ],
   "source": [
    "scores_best_asym = selected_best_LDA('asymptomatic', 'abstract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 404 abstracts selected\n"
     ]
    }
   ],
   "source": [
    "# topic number 19 is most relevant to public wearing mask\n",
    "# which topic do you think is most relevant to your search\n",
    "cor_dict_asym = select_text_from_LDA_results('asymptomatic', 'abstract', scores_best_asym, 19)\n",
    "print (\"There are {} abstracts selected\". format(len(cor_dict_asym)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 articles are relevant to the topic you choose\n"
     ]
    }
   ],
   "source": [
    "# extract relevant sentences  #search keywords can be a list\n",
    "sel_sentence_asym, sel_sentence_df_asym = extract_relevant_sentences(cor_dict_asym, ['transmission'], 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentences</th>\n",
       "      <th>sha</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>mqeg0oub</td>\n",
       "      <td>['3 Furthermore, transmission from asymptomati...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mass masking in the COVID-19 epidemic: people ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>j3avpu1y</td>\n",
       "      <td>[' No data on person-to-person or nosocomial t...</td>\n",
       "      <td>52f88e8d4b44c47b9afd25b59eb9b7f85ab95275</td>\n",
       "      <td>A familial cluster of pneumonia associated wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3w63yt7f</td>\n",
       "      <td>[' Of the 28 cases, 16 were index cases import...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Early Epidemiological and Clinical Characteris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1nhlu89c</td>\n",
       "      <td>[' However, the recent report on asymptomatic ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coronavirus disease-2019: is fever an adequate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>kwq2y3il</td>\n",
       "      <td>[' Therefore, there is still a theoretical ris...</td>\n",
       "      <td>a9a4101b25236a4fc0e14a9cbdd904ca8b2baffd</td>\n",
       "      <td>Coronavirus Disease 2019: Coronaviruses and Bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>k3f7ohzg</td>\n",
       "      <td>[' The measures to prevent transmission was ve...</td>\n",
       "      <td>14dbf1c01f2c422c1aefee32f094cc524ea03af1</td>\n",
       "      <td>Characteristics of COVID-19 infection in Beijing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>kiq6xb6k</td>\n",
       "      <td>[' Interpretation Person-to-person transmissio...</td>\n",
       "      <td>ad0e9c151402df00786e0aa6dd30987004966deb</td>\n",
       "      <td>First known person-to-person transmission of s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>626ch774</td>\n",
       "      <td>['We simulated 100 2019-nCoV infected travelle...</td>\n",
       "      <td>09e25e413faba97b87efc701d1ab8d2a18386efb; 4e55...</td>\n",
       "      <td>Effectiveness of airport screening at detectin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>pth2d40p</td>\n",
       "      <td>[' In addition, nosocomial infection of hospit...</td>\n",
       "      <td>89a8918f7e3044b89642aaa74defc7381abef482; 1f5c...</td>\n",
       "      <td>Asymptomatic carrier state, acute respiratory ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>hfkzu18p</td>\n",
       "      <td>[' Here we highlight nine most important resea...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SARS-CoV-2 and COVID-19: The most important re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          sentences  \\\n",
       "43   mqeg0oub  ['3 Furthermore, transmission from asymptomati...   \n",
       "44   j3avpu1y  [' No data on person-to-person or nosocomial t...   \n",
       "45   3w63yt7f  [' Of the 28 cases, 16 were index cases import...   \n",
       "46   1nhlu89c  [' However, the recent report on asymptomatic ...   \n",
       "47   kwq2y3il  [' Therefore, there is still a theoretical ris...   \n",
       "48   k3f7ohzg  [' The measures to prevent transmission was ve...   \n",
       "49   kiq6xb6k  [' Interpretation Person-to-person transmissio...   \n",
       "50   626ch774  ['We simulated 100 2019-nCoV infected travelle...   \n",
       "51   pth2d40p  [' In addition, nosocomial infection of hospit...   \n",
       "52   hfkzu18p  [' Here we highlight nine most important resea...   \n",
       "\n",
       "                                                  sha  \\\n",
       "43                                                NaN   \n",
       "44           52f88e8d4b44c47b9afd25b59eb9b7f85ab95275   \n",
       "45                                                NaN   \n",
       "46                                                NaN   \n",
       "47           a9a4101b25236a4fc0e14a9cbdd904ca8b2baffd   \n",
       "48           14dbf1c01f2c422c1aefee32f094cc524ea03af1   \n",
       "49           ad0e9c151402df00786e0aa6dd30987004966deb   \n",
       "50  09e25e413faba97b87efc701d1ab8d2a18386efb; 4e55...   \n",
       "51  89a8918f7e3044b89642aaa74defc7381abef482; 1f5c...   \n",
       "52                                                NaN   \n",
       "\n",
       "                                                title  \n",
       "43  Mass masking in the COVID-19 epidemic: people ...  \n",
       "44  A familial cluster of pneumonia associated wit...  \n",
       "45  Early Epidemiological and Clinical Characteris...  \n",
       "46  Coronavirus disease-2019: is fever an adequate...  \n",
       "47  Coronavirus Disease 2019: Coronaviruses and Bl...  \n",
       "48   Characteristics of COVID-19 infection in Beijing  \n",
       "49  First known person-to-person transmission of s...  \n",
       "50  Effectiveness of airport screening at detectin...  \n",
       "51  Asymptomatic carrier state, acute respiratory ...  \n",
       "52  SARS-CoV-2 and COVID-19: The most important re...  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_sentence_df_asym.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asymptomatic Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 53 articles relevant to the topic\n"
     ]
    }
   ],
   "source": [
    "#here we need to add the stats analysis \n",
    "annotation_asymptomatic = pd.read_csv(path + 'asymtomatic.csv')\n",
    "print('there are {} articles relevant to the topic'.format(annotation_asymptomatic.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 papers show that there is clear evidence show that asymtomatic cases contribute to the spread of the virus, 25 papers show that it is unlikely that asymtomatic cases contribute to the spread of the virus\n"
     ]
    }
   ],
   "source": [
    "asymptomatic = annotation_asymptomatic['stance'].value_counts()\n",
    "print('{} papers show that there is clear evidence that asymtomatic cases contribute to the spread of the virus, {} papers show that it is unlikely that asymtomatic cases contribute to the spread of the virus'.format(asymptomatic[1], asymptomatic[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Evaluation: extract keyword search entry and annotate the data for evaluation\n",
    "\n",
    "In this part, we search the data using keywords only in corressponding to each question, the keyword search result is serve as the sample for evaluation -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic searching system (baseline)\n",
    "This is a baseline search system including tfidf and cosine similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicSearch:\n",
    "    \"\"\"Basic search using tfidf and cosine similarity \"\"\"\n",
    "    def __init__(self, search_keys, varname):\n",
    "        \"\"\"Define varibles.\"\"\"\n",
    "        self.path = '/afs/inf.ed.ac.uk/user/s16/s1690903/share/cov19_2/'\n",
    "        self.search_keys = search_keys\n",
    "        self.variable = varname\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Load meta data.\"\"\"\n",
    "        dataframe = pd.read_csv(self.path + 'metadata.csv')\n",
    "        return dataframe\n",
    "\n",
    "    def tf_idf(self, search_keys, dataframe, varname):\n",
    "        \"\"\"Compute search query weights and tfidf weights.\"\"\"\n",
    "        tfidf_vectorizer = TfidfVectorizer()\n",
    "        tfidf_weights_matrix = tfidf_vectorizer.fit_transform(dataframe[varname].values.astype('U'))\n",
    "        search_query_weights = tfidf_vectorizer.transform([self.search_keys])\n",
    "\n",
    "        return search_query_weights, tfidf_weights_matrix\n",
    "\n",
    "    def cos_similarity(self, search_query_weights, tfidf_weights_matrix):\n",
    "        \"\"\"Compute cosine similarity between weights \"\"\"\n",
    "        cosine_distance = cosine_similarity(search_query_weights, tfidf_weights_matrix)\n",
    "        similarity_list = cosine_distance[0]\n",
    "\n",
    "        return similarity_list\n",
    "\n",
    "\n",
    "    def most_similar(self, similarity_list, min_talks=1):\n",
    "        \"\"\"Return entries with cosine similarity > 0 \"\"\"\n",
    "        most_similar = []\n",
    "        for idx, num in enumerate(similarity_list):\n",
    "            if num > 0:\n",
    "                most_similar.append(idx)\n",
    "\n",
    "        return most_similar\n",
    "\n",
    "    def get_search_result(self):\n",
    "        \"\"\"Get search resutls. \"\"\"\n",
    "        df = self.load_data()\n",
    "        search_query_weights, tfidf_weights_matrix = self.tf_idf(self.search_keys, df, 'abstract')\n",
    "        similarity_list = s.cos_similarity(search_query_weights, tfidf_weights_matrix)\n",
    "\n",
    "        c = s.most_similar(similarity_list, min_talks=1)\n",
    "        df['index'] = df.index\n",
    "\n",
    "        result_id = pd.DataFrame(c)\n",
    "        result_id.rename(columns={result_id.columns[0]: \"index\" }, inplace = True)\n",
    "\n",
    "        result = result_id.merge(df, on='index', how='inner')\n",
    "        #add filter title\n",
    "        result.to_csv(self.path + 'tfidf_search.csv')\n",
    "        return result\n",
    "\n",
    "    def convert_result_to_dict(self):\n",
    "        \"\"\"Convert result to dictionary. \"\"\"\n",
    "        result = self.get_search_result()\n",
    "        mydict = lambda: defaultdict(mydict)\n",
    "        result_data_dict = mydict()\n",
    "\n",
    "        for cord_uid, abstract, title, sha in zip(result['cord_uid'], result['abstract'], result['title'], result['sha']):\n",
    "            result_data_dict[cord_uid]['title'] = title\n",
    "            result_data_dict[cord_uid]['abstract'] = abstract\n",
    "            result_data_dict[cord_uid]['sha'] = sha\n",
    "\n",
    "        return result_data_dict\n",
    "\n",
    "\n",
    "    def simple_preprocess(self, result):\n",
    "        \"\"\"Simple text process: lower case, remove punc. \"\"\"\n",
    "        mydict = lambda: defaultdict(mydict)\n",
    "        cleaned = mydict()\n",
    "        for k, v in result.items():\n",
    "            sent = v[self.variable]\n",
    "            sent = str(sent).lower().translate(str.maketrans('', '', string.punctuation))\n",
    "            cleaned[k]['processed_text'] = sent\n",
    "            cleaned[k]['sha'] = v['sha']\n",
    "            cleaned[k]['title'] = v['title']\n",
    "            cleaned[k]['abstract'] = v[self.variable]\n",
    "\n",
    "        return cleaned\n",
    "\n",
    "    def extract_relevant_sentences(self, search_keywords, filter_title=None):\n",
    "        \"\"\"Extract sentences contain keyword in relevant articles. \"\"\"\n",
    "        #here user can also choose whether they would like to only select title contain covid keywords\n",
    "        result_data_dict = self.convert_result_to_dict()\n",
    "        processed_result = self.simple_preprocess(result_data_dict)\n",
    "\n",
    "        mydict = lambda: defaultdict(mydict)\n",
    "        sel_sentence = mydict()\n",
    "        filter_w =  ['covid19','ncov','2019-ncov','covid-19','sars-cov','wuhan']\n",
    "        \n",
    "        for k, v in processed_result.items():\n",
    "            keyword_sentence = []\n",
    "            sentences = v['abstract'].split('.')\n",
    "            for sentence in sentences:\n",
    "                # for each sentence, check if keyword exist\n",
    "                # append sentences contain keyword to list\n",
    "                keyword_sum = sum(1 for word in search_keywords if word in sentence.lower())\n",
    "                if keyword_sum > 0:\n",
    "                    keyword_sentence.append(sentence)\n",
    "\n",
    "            # store results\n",
    "            if not keyword_sentence:\n",
    "                pass\n",
    "            elif filter_title is not None:\n",
    "                for f in filter_w:\n",
    "                    title = v['title'].lower().translate(str.maketrans('', '', string.punctuation))\n",
    "                    abstract = v['abstract'].lower().translate(str.maketrans('', '', string.punctuation))\n",
    "                    if (f in title) or (f in abstract):\n",
    "                        sel_sentence[k]['sentences'] = keyword_sentence\n",
    "                        sel_sentence[k]['sha'] = v['sha']\n",
    "                        sel_sentence[k]['title'] = v['title']\n",
    "            else:\n",
    "                sel_sentence[k]['sentences'] = keyword_sentence\n",
    "                sel_sentence[k]['sha'] = v['sha']\n",
    "                sel_sentence[k]['title'] = v['title']\n",
    "\n",
    "        print('{} articles are relevant to the topic you choose'.format(len(sel_sentence)))\n",
    "\n",
    "        path = '/afs/inf.ed.ac.uk/user/s16/s1690903/share/cov19_2/search_results/'\n",
    "        df = pd.DataFrame.from_dict(sel_sentence, orient='index')\n",
    "        df.to_csv(path + 'tfidf_results_{}.csv'.format(search_keywords))\n",
    "        sel_sentence_df = pd.read_csv(path + 'tfidf_results_{}.csv'.format(search_keywords))\n",
    "        return sel_sentence, sel_sentence_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we retrieve the data for baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 articles are relevant to the topic you choose\n"
     ]
    }
   ],
   "source": [
    "s = BasicSearch('wear mask', 'abstract') #enter query\n",
    "result = s.extract_relevant_sentences(['mask']) #extract sentence contain this keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205 articles are relevant to the topic you choose\n"
     ]
    }
   ],
   "source": [
    "s = BasicSearch('coronavirus disappear in summer', 'abstract') #enter query\n",
    "result = s.extract_relevant_sentences(['summer']) #extract sentence contain this keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157 articles are relevant to the topic you choose\n"
     ]
    }
   ],
   "source": [
    "s = BasicSearch('incubation period', 'abstract') #enter query\n",
    "result = s.extract_relevant_sentences(['incubation', 'day'], 'title') #extract sentence contain this keyword and title mention covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168 articles are relevant to the topic you choose\n"
     ]
    }
   ],
   "source": [
    "s = BasicSearch('asymptomatic patients contagious', 'abstract') #enter query\n",
    "result = s.extract_relevant_sentences(['transmission'], 'title') #extract sentence contain this keyword and title mention covid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## matching labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/afs/inf.ed.ac.uk/user/s16/s1690903/share/cov19_2/annotation/'\n",
    "incu1 = pd.read_csv(path + 'incubation.csv')\n",
    "incu2 = pd.read_csv(path + 'incubation2.csv')\n",
    "\n",
    "incu1.rename(columns={incu1.columns[0]: \"textid\" }, inplace = True)\n",
    "incu2.rename(columns={incu2.columns[0]: \"textid\" }, inplace = True)\n",
    "incu = incu1.merge(incu2, on ='textid', how='inner')\n",
    "incu.to_csv(path + 'incubation_adjusted.csv')\n",
    "incu2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
